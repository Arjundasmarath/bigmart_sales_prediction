{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92046d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Sample:\n",
      "  Item_Identifier  Item_Weight Item_Fat_Content  Item_Visibility  \\\n",
      "0           FDA15         9.30          Low Fat         0.016047   \n",
      "1           DRC01         5.92          Regular         0.019278   \n",
      "2           FDN15        17.50          Low Fat         0.016760   \n",
      "3           FDX07        19.20          Regular         0.000000   \n",
      "4           NCD19         8.93          Low Fat         0.000000   \n",
      "\n",
      "               Item_Type  Item_MRP Outlet_Identifier  \\\n",
      "0                  Dairy  249.8092            OUT049   \n",
      "1            Soft Drinks   48.2692            OUT018   \n",
      "2                   Meat  141.6180            OUT049   \n",
      "3  Fruits and Vegetables  182.0950            OUT010   \n",
      "4              Household   53.8614            OUT013   \n",
      "\n",
      "   Outlet_Establishment_Year Outlet_Size Outlet_Location_Type  \\\n",
      "0                       1999      Medium               Tier 1   \n",
      "1                       2009      Medium               Tier 3   \n",
      "2                       1999      Medium               Tier 1   \n",
      "3                       1998         NaN               Tier 3   \n",
      "4                       1987        High               Tier 3   \n",
      "\n",
      "         Outlet_Type  Item_Outlet_Sales  \n",
      "0  Supermarket Type1          3735.1380  \n",
      "1  Supermarket Type2           443.4228  \n",
      "2  Supermarket Type1          2097.2700  \n",
      "3      Grocery Store           732.3800  \n",
      "4  Supermarket Type1           994.7052  \n",
      "\n",
      "Test Data Sample:\n",
      "  Item_Identifier  Item_Weight Item_Fat_Content  Item_Visibility    Item_Type  \\\n",
      "0           FDW58       20.750          Low Fat         0.007565  Snack Foods   \n",
      "1           FDW14        8.300              reg         0.038428        Dairy   \n",
      "2           NCN55       14.600          Low Fat         0.099575       Others   \n",
      "3           FDQ58        7.315          Low Fat         0.015388  Snack Foods   \n",
      "4           FDY38          NaN          Regular         0.118599        Dairy   \n",
      "\n",
      "   Item_MRP Outlet_Identifier  Outlet_Establishment_Year Outlet_Size  \\\n",
      "0  107.8622            OUT049                       1999      Medium   \n",
      "1   87.3198            OUT017                       2007         NaN   \n",
      "2  241.7538            OUT010                       1998         NaN   \n",
      "3  155.0340            OUT017                       2007         NaN   \n",
      "4  234.2300            OUT027                       1985      Medium   \n",
      "\n",
      "  Outlet_Location_Type        Outlet_Type  \n",
      "0               Tier 1  Supermarket Type1  \n",
      "1               Tier 2  Supermarket Type1  \n",
      "2               Tier 3      Grocery Store  \n",
      "3               Tier 2  Supermarket Type1  \n",
      "4               Tier 3  Supermarket Type3  \n"
     ]
    }
   ],
   "source": [
    "## Sales Prediction for Big Mart Outlets\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "\n",
    "# Set the working directory to the current directory\n",
    "script_dir = os.getcwd()\n",
    "os.chdir(script_dir)\n",
    "\n",
    "# Load the datasets\n",
    "train = pd.read_csv('train_df.csv')\n",
    "test = pd.read_csv('test_df.csv')\n",
    "sample_submission = pd.read_csv('sample_submission_df.csv')\n",
    "\n",
    "print(\"Train Data Sample:\")\n",
    "print(train.head())\n",
    "print(\"\\nTest Data Sample:\")\n",
    "print(test.head())\n",
    "\n",
    "# --- Feature Engineering ---\n",
    "if 'Item_Visibility' in train.columns:\n",
    "    train['Item_Visibility_MeanRatio'] = train['Item_Visibility'] / train['Item_Visibility'].mean()\n",
    "    test['Item_Visibility_MeanRatio'] = test['Item_Visibility'] / train['Item_Visibility'].mean()\n",
    "if 'Outlet_Establishment_Year' in train.columns:\n",
    "    train['Outlet_Years'] = 2025 - train['Outlet_Establishment_Year']\n",
    "    test['Outlet_Years'] = 2025 - test['Outlet_Establishment_Year']\n",
    "if 'Item_Outlet_Sales' in train.columns:\n",
    "    train['Item_Outlet_Sales'] = np.log1p(train['Item_Outlet_Sales'])\n",
    "if 'Item_Visibility' in train.columns:\n",
    "    train['Item_Visibility'] = np.log1p(train['Item_Visibility'])\n",
    "    test['Item_Visibility'] = np.log1p(test['Item_Visibility'])\n",
    "if 'Item_Type' in train.columns and 'Outlet_Type' in train.columns:\n",
    "    train['Item_Outlet_Combo'] = train['Item_Type'] + '_' + train['Outlet_Type']\n",
    "    test['Item_Outlet_Combo'] = test['Item_Type'] + '_' + test['Outlet_Type']\n",
    "\n",
    "def preprocess_data(df):\n",
    "    for col in df.select_dtypes(include=[np.number]).columns:\n",
    "        df[col].fillna(df[col].mean(), inplace=True)\n",
    "    for col in df.select_dtypes(include=['object']).columns:\n",
    "        df[col].fillna(df[col].mode()[0], inplace=True)\n",
    "    df = pd.get_dummies(df, drop_first=True)\n",
    "    return df\n",
    "\n",
    "train_processed = preprocess_data(train)\n",
    "test_processed = preprocess_data(test)\n",
    "\n",
    "X = train_processed.drop('Item_Outlet_Sales', axis=1)\n",
    "y = train_processed['Item_Outlet_Sales']\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9eaf8ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest CV Mean Squared Error: 0.32\n",
      "RandomForest Validation MSE: 0.30\n",
      "RandomForest Validation MSE: 0.30\n",
      "XGBoost CV Mean Squared Error: 0.29\n",
      "XGBoost CV Mean Squared Error: 0.29\n",
      "XGBoost Validation MSE: 0.28\n",
      "XGBoost Validation MSE: 0.28\n"
     ]
    }
   ],
   "source": [
    "# --- Advanced Models: RandomForest & XGBoost ---\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_scores = cross_val_score(rf, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "print(f\"RandomForest CV Mean Squared Error: {-rf_scores.mean():.2f}\")\n",
    "rf.fit(X_train, y_train)\n",
    "rf_val_pred = rf.predict(X_val)\n",
    "rf_mse = mean_squared_error(y_val, rf_val_pred)\n",
    "print(f\"RandomForest Validation MSE: {rf_mse:.2f}\")\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "xgb_scores = cross_val_score(xgb_model, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "print(f\"XGBoost CV Mean Squared Error: {-xgb_scores.mean():.2f}\")\n",
    "xgb_model.fit(X_train, y_train)\n",
    "xgb_val_pred = xgb_model.predict(X_val)\n",
    "xgb_mse = mean_squared_error(y_val, xgb_val_pred)\n",
    "print(f\"XGBoost Validation MSE: {xgb_mse:.2f}\")\n",
    "\n",
    "test_processed_aligned = test_processed.reindex(columns=X_train.columns, fill_value=0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3c63381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file created with XGBoost predictions: submission.csv\n"
     ]
    }
   ],
   "source": [
    "# --- Final Submission with XGBoost ---\n",
    "xgb_test_pred = xgb_model.predict(test_processed_aligned)\n",
    "xgb_test_pred = np.expm1(xgb_test_pred)\n",
    "xgb_test_pred = np.clip(xgb_test_pred, 0, None)\n",
    "submission = sample_submission.copy()\n",
    "submission['Item_Outlet_Sales'] = xgb_test_pred\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"Submission file created with XGBoost predictions: submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8455f6d7",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning and Stacking Ensemble\n",
    "Tune XGBoost hyperparameters and build a stacking ensemble for improved performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7911b9b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "Best XGBoost Params: {'subsample': 0.8, 'reg_lambda': 2, 'reg_alpha': 0, 'n_estimators': 300, 'max_depth': 5, 'learning_rate': 0.05, 'colsample_bytree': 0.6}\n",
      "Best XGBoost CV Score: 0.2806818058648225\n",
      "Stacking Validation MSE: 0.27\n",
      "Submission file created with stacking predictions: submission_stack.csv\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning for XGBoost\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "xgb_params = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 5, 7, 10],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'reg_alpha': [0, 0.1, 0.5],\n",
    "    'reg_lambda': [1, 1.5, 2]\n",
    "}\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(random_state=42, n_jobs=-1)\n",
    "random_search = RandomizedSearchCV(xgb_model, xgb_params, n_iter=20, scoring='neg_mean_squared_error', cv=3, verbose=1, random_state=42)\n",
    "random_search.fit(X_train, y_train)\n",
    "print('Best XGBoost Params:', random_search.best_params_)\n",
    "print('Best XGBoost CV Score:', -random_search.best_score_)\n",
    "\n",
    "# Stacking ensemble\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "stack = StackingRegressor(\n",
    "    estimators=[\n",
    "        ('xgb', xgb.XGBRegressor(**random_search.best_params_, random_state=42, n_jobs=-1)),\n",
    "        ('rf', rf)\n",
    "    ],\n",
    "    final_estimator=Ridge(random_state=42)\n",
    ")\n",
    "stack.fit(X_train, y_train)\n",
    "y_pred_stack = stack.predict(X_val)\n",
    "stack_mse = mean_squared_error(y_val, y_pred_stack)\n",
    "print(f'Stacking Validation MSE: {stack_mse:.2f}')\n",
    "\n",
    "# Predict on test set using stacking ensemble\n",
    "test_pred_stack = stack.predict(test_processed_aligned)\n",
    "test_pred_stack = np.expm1(test_pred_stack)\n",
    "test_pred_stack = np.clip(test_pred_stack, 0, None)\n",
    "submission_stack = sample_submission.copy()\n",
    "submission_stack['Item_Outlet_Sales'] = test_pred_stack\n",
    "submission_stack.to_csv('submission_stack.csv', index=False)\n",
    "print('Submission file created with stacking predictions: submission_stack.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2377a3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 XGBoost Features:\n",
      "                                                feature  importance\n",
      "1586                           Outlet_Identifier_OUT019    0.083036\n",
      "1585                           Outlet_Identifier_OUT018    0.075102\n",
      "1598                      Outlet_Type_Supermarket Type3    0.039044\n",
      "5                                          Outlet_Years    0.037980\n",
      "1596                      Outlet_Type_Supermarket Type1    0.037894\n",
      "1592                                 Outlet_Size_Medium    0.034382\n",
      "2                                              Item_MRP    0.031452\n",
      "1587                           Outlet_Identifier_OUT027    0.029830\n",
      "1622  Item_Outlet_Combo_Fruits and Vegetables_Grocer...    0.029265\n",
      "3                             Outlet_Establishment_Year    0.027078\n",
      "1634          Item_Outlet_Combo_Household_Grocery Store    0.026009\n",
      "1618       Item_Outlet_Combo_Frozen Foods_Grocery Store    0.020963\n",
      "1610             Item_Outlet_Combo_Canned_Grocery Store    0.009451\n",
      "1650        Item_Outlet_Combo_Snack Foods_Grocery Store    0.009328\n",
      "988                               Item_Identifier_FDU21    0.008746\n",
      "1595                        Outlet_Location_Type_Tier 3    0.007786\n",
      "109                               Item_Identifier_DRK12    0.007774\n",
      "858                               Item_Identifier_FDR43    0.006564\n",
      "1597                      Outlet_Type_Supermarket Type2    0.005161\n",
      "1593                                  Outlet_Size_Small    0.005136\n",
      "Stacking (Top Features) Validation MSE: 0.27\n",
      "Submission file created with stacking predictions (top features): submission_stack_top.csv\n"
     ]
    }
   ],
   "source": [
    "# Feature Selection using XGBoost Feature Importances\n",
    "importances = random_search.best_estimator_.feature_importances_\n",
    "feature_names = X_train.columns\n",
    "feature_importance_df = pd.DataFrame({'feature': feature_names, 'importance': importances})\n",
    "feature_importance_df = feature_importance_df.sort_values(by='importance', ascending=False)\n",
    "print('Top 20 XGBoost Features:')\n",
    "print(feature_importance_df.head(20))\n",
    "\n",
    "# Select top N important features\n",
    "top_features = feature_importance_df['feature'].head(20).tolist()\n",
    "X_train_top = X_train[top_features]\n",
    "X_val_top = X_val[top_features]\n",
    "test_top = test_processed_aligned[top_features]\n",
    "\n",
    "# Retrain stacking on selected features\n",
    "stack_top = StackingRegressor(\n",
    "    estimators=[\n",
    "        ('xgb', xgb.XGBRegressor(**random_search.best_params_, random_state=42, n_jobs=-1)),\n",
    "        ('rf', RandomForestRegressor(n_estimators=100, random_state=42))\n",
    "    ],\n",
    "    final_estimator=Ridge(random_state=42)\n",
    ")\n",
    "stack_top.fit(X_train_top, y_train)\n",
    "y_pred_stack_top = stack_top.predict(X_val_top)\n",
    "stack_top_mse = mean_squared_error(y_val, y_pred_stack_top)\n",
    "print(f'Stacking (Top Features) Validation MSE: {stack_top_mse:.2f}')\n",
    "\n",
    "test_pred_stack_top = stack_top.predict(test_top)\n",
    "test_pred_stack_top = np.expm1(test_pred_stack_top)\n",
    "test_pred_stack_top = np.clip(test_pred_stack_top, 0, None)\n",
    "submission_stack_top = sample_submission.copy()\n",
    "submission_stack_top['Item_Outlet_Sales'] = test_pred_stack_top\n",
    "submission_stack_top.to_csv('submission_stack_top.csv', index=False)\n",
    "print('Submission file created with stacking predictions (top features): submission_stack_top.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2381857",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4eb9d7c2",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA) and Feature Engineering Rationale\n",
    "This section provides an overview of the EDA performed on the Big Mart sales dataset, including key insights and the reasoning behind each feature engineering transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb30268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA: Data Overview and Initial Visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Display basic info\n",
    "print('Train shape:', train.shape)\n",
    "print('Test shape:', test.shape)\n",
    "print('Train columns:', train.columns.tolist())\n",
    "print('Test columns:', test.columns.tolist())\n",
    "\n",
    "# Missing values\n",
    "print('Missing values in train:')\n",
    "print(train.isnull().sum())\n",
    "print('\\nMissing values in test:')\n",
    "print(test.isnull().sum())\n",
    "\n",
    "# Target distribution\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.histplot(train['Item_Outlet_Sales'], bins=50, kde=True)\n",
    "plt.title('Target Distribution: Item_Outlet_Sales')\n",
    "plt.show()\n",
    "\n",
    "# Feature distributions\n",
    "num_features = ['Item_Weight', 'Item_Visibility', 'Item_MRP']\n",
    "for col in num_features:\n",
    "    plt.figure(figsize=(6,3))\n",
    "    sns.histplot(train[col], bins=40, kde=True)\n",
    "    plt.title(f'Distribution of {col}')\n",
    "    plt.show()\n",
    "\n",
    "# Categorical feature counts\n",
    "cat_features = ['Item_Type', 'Outlet_Type', 'Outlet_Size', 'Outlet_Location_Type']\n",
    "for col in cat_features:\n",
    "    plt.figure(figsize=(8,3))\n",
    "    sns.countplot(y=train[col])\n",
    "    plt.title(f'Counts of {col}')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
